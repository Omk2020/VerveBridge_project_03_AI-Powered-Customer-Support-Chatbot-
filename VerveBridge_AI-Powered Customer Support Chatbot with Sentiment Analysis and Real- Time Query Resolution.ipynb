{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a22215d6-d70d-44ca-9935-a0fc7d17b607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123 MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Track my order #123\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a271912f-cdc4-40c9-9366-e9f2795dee6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b16cdfce-197d-41c5-8904-79ba50f8fe40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998701810836792}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Shared/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "# Disable tokenizers parallelism to avoid the warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Define the sentiment analysis pipeline\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "revision = \"af0f99b\"\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model_name, revision=revision)\n",
    "\n",
    "# Example usage without clean_up_tokenization_spaces argument\n",
    "result = sentiment_pipeline(\"I love programming!\")\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a860053e-8534-4a2a-9fa7-c8018eafe04c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26021e23-7da9-41eb-bb4d-f15f6aff1f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.4, 'pos': 0.6, 'compound': 0.6696}\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "sentiment = analyzer.polarity_scores(\"I love this product!\")\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c3b2ca-3bff-4722-9fa9-d8fb6889beb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddcef0fe-96f3-4102-82ea-325e57a268e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.625, subjectivity=0.6)\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "blob = TextBlob(\"I love this product!\")\n",
    "print(blob.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba10386b-d035-4f80-b954-0287f36b25fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c7eb19-c032-4b99-91ea-849a90ec3716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a7314b-c31d-4eb2-a3fe-30bc7b68e66d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ba8709-c899-437f-8dc2-098b1cedc541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddde766-306a-4cac-a16c-65f8a8f6ff0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f07dd8e-9537-4ecf-b54c-c7649524893a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, it became possible if nobody knew a good way to connect.\n",
      "\n",
      "Since the arrival of the Internet, two of the very first people who wrote about \"how the Internet worked\" have all come out with their own theories and\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the model\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "# Generate text\n",
    "prompt = \"Once upon a time\"\n",
    "results = generator(prompt, max_length=50)\n",
    "\n",
    "print(results[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eee1182-6995-4f44-9ce7-f0525c64f6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21c9213d-8c74-4109-9d08-80d460abeb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "user_item_matrix = np.array([\n",
    "    [5, 4, 0, 0],\n",
    "    [4, 0, 3, 2],\n",
    "    [1, 1, 0, 5],\n",
    "])\n",
    "\n",
    "model = NearestNeighbors(metric='cosine', n_neighbors=3)  # Set n_neighbors to 3\n",
    "model.fit(user_item_matrix)\n",
    "\n",
    "distances, indices = model.kneighbors(user_item_matrix[0].reshape(1, -1))\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d11452-fb75-44d8-ab03-cd8c4254aad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46f3ba8d-336f-4c06-b8aa-1708ca595dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate a professional response to a customer asking for help with tracking their order.\n",
      "\n",
      "Fully automate your job\n",
      ". Automate your tasks, from getting the order in time to getting it delivered to your customer. Get notified of the time your\n"
     ]
    }
   ],
   "source": [
    "# Refined prompt\n",
    "prompt = \"Generate a professional response to a customer asking for help with tracking their order.\"\n",
    "\n",
    "# Tokenize input with padding and attention mask\n",
    "inputs = tokenizer(prompt, return_tensors=\"tf\", padding=True)\n",
    "\n",
    "# Generate output with sampling enabled\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=50,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    no_repeat_ngram_size=2,\n",
    "    temperature=0.7,  # Control randomness\n",
    "    do_sample=True    # Enable sampling to make temperature effective\n",
    ")\n",
    "\n",
    "# Decode and print the generated output\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143ee450-dcff-4cf4-bab6-28f7317af36d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa35e3b8-d154-4fe4-b33a-82dedfb8c62a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bac67be-d600-4597-a565-26b0ef27713f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5250e70-e1fc-4301-9ba1-8a893ca5dda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate a response for customer query.\n",
      "\n",
      "The following code snippet will generate a new response:\n",
      ".response('Hello World ', {'name':'John'});\n",
      " (Note: this code is only for the Java version\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set the pad token as the end of sequence token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the model\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Tokenize input with padding and attention mask\n",
    "inputs = tokenizer(\"Generate a response for customer query.\", return_tensors=\"tf\", padding=True)\n",
    "\n",
    "# Generate output, ensuring the attention mask is passed and `pad_token_id` is set\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=50,\n",
    "    pad_token_id=tokenizer.eos_token_id,  # Use eos_token_id as the padding token\n",
    "    no_repeat_ngram_size=2  # Prevent repetitive text generation\n",
    ")\n",
    "\n",
    "# Decode and print the generated output\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7435adb-b1d7-4bf3-90fb-175f787ebfdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad536335-bfd3-4e44-8a23-ee3d4e70bf0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47d173aa-727b-4362-8915-74a244d6876d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample queries\n",
    "queries = [\"Track my order\", \"Order status\", \"Product details\", \"Cancel my order\"]\n",
    "\n",
    "# Initialize a TfidfVectorizer to convert text to numerical form\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the queries into vectors\n",
    "vectorized_queries = vectorizer.fit_transform(queries)\n",
    "\n",
    "# Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(vectorized_queries)\n",
    "\n",
    "# Output the labels (clusters) for each query\n",
    "print(kmeans.labels_).0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5eb793-800d-4c06-8c9a-77166f653f37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
